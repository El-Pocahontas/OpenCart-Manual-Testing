Module 1: Software testing concepts (Theory - What?)
Module 2: Test Project (Practical - How?)
Module 3: Agile process - Jira

Module 1

1) What us Software? Types of Softwares?

A Software is a collection of computer programs that helps us to perform a task.

Types of Softwares:

A) System software - device drives, Operating Systems, Servers, Utilities, etc.
B) Programing software - Compilators, debuggers interpreters, etc.
C) Application software - Web Applications, Mobile Apps, Desctops Applications, etc.

2) What is Software testing?

Example - X bank(company) -> IT company -> Develop -> Test -> Deliver -> X Bank

Software Testing is part of software development process.
Software Testing is an activity to detect and identify the defects in the software.
The obective testing is to relise quality product to the client.

3) Software Quality

Bug-free.
Delivered on time.
Within budget.
Meets requirements and/or expectations.
Maintainable.

4) Project vs Product

If software application is developed for specific customer based on reqirement then it is called Project.
If software application is developed for multiple customers based on market requirements then it called Product.

5) Why we need Testing?

Deliver quality produkt.

6) Error, Bug/Defect & Failure

Error - human mastake (Developer)
Bug - deveation from expectations/requirements (Tester)
Failure - if something not working in customer environment (Customer)

7) Why the software has bugs?

Miscommunication or no comunication
Software complexity
Programming errors
Changing requirements
Lack of skilled testers

8) SDLC - Software Development Life Cycle

Software Development Life Cycle is proces used by software industry to design, develop and test software.

P - People
P - Process
P - Product

Requirements - Analysis - Collect and uderstend requirements
Design - Designers design software how it should look like
Development - Develop by using diferent type of lenguages
Testing - Testers (QA, QC)
Maintenance - Deliver and using product by customer

9) Waterfall Model (old, traditional)

Requirements - collect, prepare documents, talk to castomer, project managers
	System design - heigh & low level modules design, design documents
		Implementation - Coding based on documentacion
			Testing - test plan, test cases, user manual
				Deployment - install in customer environment
					Maintenance - using by customer
Every level need date from level before. Input - outcome.
Every faze depend on previes.

Advantages
A) Quality of product will be good. - clear documentation
B) Since Requirements are not allowed, chances of finding bugs will be less.
C) Initial investment is less since the testers are hired at the later stages.
D) Preferred for small projects where requirements are freezed.

Disadvantages
A) Requirements changes are not allowed.
B) If there is defect in requirements that will continued in later phases.
C) Total investment is more, becose time taking for rework on defect id time consuming which leads to high investment.
D) Testing will start only after coding.

10) Spiral model

Planning 						Risk Analysis


	2 Requirement Analysis	>		3Prototype >

				1 Cost >

	4 Customer Evaluation			5 Development & Testing >



Evaluation						Engineering & Execution


1 Cost > 2 Requirement Analysis	> 3 Prototype > 4 Customer Evaluation > 5 Development & Testing > 1 Cost …

Every cycle we relise peace of software to custommer.

Advantages
A) Spiral model is iterative model.
B) Spiral Model overcome drowbacks of Waterfall model.
C) We follow spiral model whenever there is dependency on the modules.
D) I every cycle new software will be released to customer.
E) Software will be relised in multiple versions. So it also called version control model.

Disadvantages
A) Requirements changes are not allowed in between the cycle.
B) Every cycle of spiral model looks like waterfall.
C) There is no testingin requirements & design phase.

Phrototype - blueprint of the software.(between Waterfall & Spiral)
Initial requirements from the customer -> Prototype -> Customer -> design, coding, testing…

Example.

Gmail: Login > Inbox > Compose > Sent > Recive email (moduls - part of software)
Bank: Login, Check balance, Fund transfer, Req statment, add payee

11) V - Model/ VV Model (verification & validation) testing in every phase

Verification 							Validation

BRS/CRS/URS -bisnes req. preper documents			User Acceptance Testing
	SRS						System Testing
		HLD				Integration Testing
			LED's		Unit Testing
				Coding

BRS/CRS/URS + SRS - review (document is checked - correct and completed) testing documentation

Technic to verify documents (static testing - testing the project related document):
Review 
Walkthrought
Inspection

Softwer devided in small moduls
4 testing under are dynamic testing
Unit testing - testing one modul 
Integration testing - after completed unit testing for all moduls
^ both will be tested by developers (White box testing - include coding - internal)

System testing - funtionality, working, by testers (black box testing - with no code)

User acceptence testing - testers & customers (also black box testing)

Verification - Checks whether we are building the right product, Focus on documentation, involves Review, Walkthrought, Inspection

Validation - checks wheter we are building the product right, takes place after verifications are completed, Focus on Software, involves actual testing, unit, integration, system & UAT testing

s1 -> s2 -> s3 -> Application

Advanteges
A) Testing involved in each and every phase.

Disadvantages
A) Documentation is more.
B) Initial investment is more.

Review - Conducts on documents to ensure correctness ant completness.

Requiirement review
Design review
Code review
Test plan review
Test cases review

Walkthrough - informal review (no plan)
Author reads the documents or code and discuss with peers.
It's pre-planned and can be done whenever required.
Also walkthrough does not have minutes of the meet.

Inspection

Most mormal review type.
In which at last 3-8 people will sit in the meeting 1- reader 2 writer 3 moderator plus  concerned.
Inspection will have proper schedule which will be intimated via email to the concerned developer/tester.

QA vs QC

QA is Process related. Every step of SDLC
QC is the actual testing oc the software. Only in Testing.

QA focused on building in quality.
QC focuses on testing for quality.

QA is preventing defects.
QC is detecting defects.

QA is proces oriented.
QC is product oriented.

QA for entire SDLC.
QC for testing part in SDLC.

QE - Quality Engineering. Write code to test the software.

Software Engineer - SE Write code to develope the software.

Levels of testing

Unit testing - Testing small part of code (by developer) White Box
Integration Testing - More parts of moduls by developer White Box
System Testing
User Acceptance Testing (UAT)

Unit Testing

A unit is a single component of module of software.
Unit testing conducts on a single program or single module.
Unit testing is white box testing technique.
Unit testing techniques:
Basis path testing - every line of code program should be exequted at least once
Control structure testing  
Conditional coverage - giving data (conditons) and check
Loops coverage - verifaing loops
Mutation Testing - repetation

Integration testing

performer between 2 or more moduls.
focuses on cheking data communication between multiple moduls.
is whie box testing technique.

Types of integration testing
1) Incremental - incrementally adding the modules and testing the data flow beetween the modules. (2 Approaches:Top Down, Buttom Up)
Top down - incrementally adding the modules and testing the data flow beetween the modules. and ensure the module added is the child of previous module.
Buttom Up - incrementally adding the modules and testing the data flow beetween the modules. and ensure the module added is the parent of previous module.
Sandwich/Hybrid Approach - Combination of Top Down & Bottom Up.

2) Non-incremental - Adding all the modules in a single shot and test the data flow between modules.

Drowbacks: 
We might miss data flow between some of the modules.
If you find any defect we can't understand the root couse of defect.

System Testing (for testers)

Testing over all funtionality of the application with respective client requirements.
It is a black box testing technique.
Testing is conducted by testing team.
After completion of component and integretion level testing's we start System testing.
Before conducting system testing we should know the customer requirements.

System testing focusses on below aspect:
User interface Testing (GUI)
Functional testing
Non-functional testing
Usability testing

User Acceptence Testing (UAT)

After compilation testing UAT team conducts acceptence testing in two levels.
Alpha testing
Beta Testing
-------

GUI testing
Grafical user-interface testing is a proces of testing the user interface of an application.
GUI includes all the elements such as menus, checkbox, buttons, colors, fonts, sizes, icons, content and imeges.

GUI testing Checklist:
Testing…
the size , position, width, height of the elements.
of the error messages that are getting displayed.
the different section of the screen.
of the screen in diffrent resolutions with the help of zooming in and zooming out.
the alignment of texts and other elements like icons, buttonsm etc. are in proper place or not.
the colors of the fonts.
whether the image has good clarity or not.
the alignment of the images.
of ten spelling.
the user must not ger furstrated while using the system interface.
whether the interface is attractive or not.
of the scrollbars according to the size of the page if any.
of the disaveled fields if any.
of the headings whether it is properly aligned or not.
of the color of the hyperlink.
UI Elements like buttoln, textbox, text area, check box, radio buttons, drop downs, links etc.

Usability testing
During this testing validates application provided context sensitive help or not to the user. 
Checks how easily the end users are able to understand and operate the application is called usability.

Functional testing
Functionality is nothing but behawior of application.
Functional testing talks about how ypur feature should work.
---------

Object Properties testing - example: Enable, disable, visable, Focus...

Database Testing(Backend) DML Operations (DataManipulationLanguage): insert, update, delete, select =SQL= 
Whitebox Part
[Table & Column level validations(column type, column lenghth, numer of columns…
Relation between the tables (Normalization)
Functions
Precedures
Triggers
Indexes
Views
etc...]

Error Handling
Testers verify the messages while performing incorrect actions on the application.
Error messages should be readable.
User unerstandable/simple language.

Calcutations/Manipulations Testing
Tester should verify the calculations.

Links Extence & Links Execution
Where exactly the linkst are placed - link existace
Links are navigating to proper page or not - links execution

Inernal links
External links
Broken links

Cookies & Sessions
Cookies - temporary files created by browser while browsing the pages through internet.
Sessions are time slots created by the server. Sessions will be expired after come time (if you are idle for some time)
---

Non-functional testing
Once the application functionality is stable then we do non-functional testing.
Focus on performence, load it can take and security etc.
-
Performence testing
-Load testing
-Stress Testing
-Volume Testing
Security testing
Recovery testing
Compatibility testing
Configuration Testing
Installation testing
Sanitation/Garbage testing

Non-functional testing
---
Performance - speed
- Load - increasing the load on the application slowly then check the speed of the application.
- Stress - suddenly increase/decrease the load on the application and check speed of the application.
- Volume - check how much data is able to handle by the application.

Security - how secure is our application.
Authentication - users are valid or not
Autherization/Acces Control - permissions of valid user

Recovery testing:
check the system change to abnormal to normal.

Compatibility testing:
Forward Compatibility
Backward Compatibility
Hardware Compatibility (configuration testing)

Installation testing
Check screens are clear to understand.
Simple or not
Un-installation.

Sanitation/Garbage testing:
If any application provides extra features/functionality than we consider tchem as bug.

-------------------------------------------------------------------------

Regresion testing

Bild (ex. 4 moduls)> find bug > send to developer > new bild > change should not inpact other moduls > tester verify changed modul and dependent moduls

Testing conducts on modified build to make sure there will not be inpact on existing funcionality becouse of changes like adding/deleting/modifying features.

3 typer of rgresion testing:
1. Unity regression testing - testing only the changes done by developer.
2. Regional regression testing - testing modified module along with the inpacted modules. Inpact Analysis meeting conducts to identify impacted modules with QA & Dev.
3. Full regression - Testing the main feature & remaining part of the application. Ex: Dev hest done changes in many modules, insted of identifying inpacted modules, we perform one round of full regression.
-------------

Re-Testing

Bug > developer > new build > re-testing (check if bug was fixed)

Tester clese the bug if it worked otherwise re-open and send to developer.
To ensure that defect which were found and posted in the earlier build were fixed or not in the current build.
---------

Smoke vs Sanity Testing

Smoke:(instellation, completed, basic ex. home page)
Smoke test is done to make sure the build we recived from the development team id testable/stable or not. 
Is performer by Developers and testers.
Build can by stable or unstable.
It is done on initial builds.
It is part of basic testing.
Usually it is done every time there is a new build relese.

Sanity: (main functionality)
Sanity test is done during the relese phase to check for the main functionalities of the application without going deeper.
Is performer by testers alone.
Build is relativly stable.
It is done on stable builds.
It is part of regression testing.
It is planned when there is no enought time to do in-depth testing.
---------

Exploratory testing
We have explote the application, understand completly and test it.
Understand the application, identify all possible scenarios, document it then use it for testing.
We do exploratory testing when Application is ready byt there is no rewirement.

Drawbacks:
You might misunderstand any feature as a bug or any bug as feature since ypu do not have requirement.
Time consuming.
If there is any bug in applicetion, ypu will never know about it.
------

Adhoc testing

Testing application randomly without any test cases or any buissnes requirement document.
Adhoc testing is an infoirmal testing type with an aim to break the system.
Tester should have knoledge of application even thou he doesn't have requirements/test cases.
This test is usually an unplanned activity.
-----

Monkey/Gorilla testing

Testing application randomly without any test cases or any buisness requirement document.
Informal testing type with an aim to break the system.
Tester do not have knowledge of application.
Suitable for gaming application.
------

Positive testing
Testing the application with valid inputs is called Positiv Testing.
It checks whether an application behaves as expected with positiv inputs.
Exp.  Box where ypu can put numbers in range 0 to 999. Set vaild input values and check wheder the system is accepting the values.

Negativ testing
Tasting application with invalid inputs is called Negativ Testing.
It checks wheter an application behaves as expected with the negative inputs.
Exp. You trying to put letters from a to z and chceck if applicention willa accept it or not or throw an error message.
-------

End to End testing
Testing overall functionality of the system including the data integration among all the modules is called and to end testing.
Exp. Login > add new customer > Edit customer > Delete customer > logout
---------

Globalization and localization testing

Globalization testing:(alsaw writen as I18N testing)
Performed to ensure the system or software application can run in any cultural or local environment.
Different aspects of software application are tested to ensure that it supports every lenguage and different attributes.
It tests the different currency formats, mobile numer formats and address formats are supported by the application.

Localation testing:
Performed to check system or application for a specific  graphical and cultural environment.
Localized product only supports the specific kind of lenguage and is usable only in specific region.
It testes the specific currency format, mobile numer format and adress format is working properly or not.
-----------

Test Design Techniques/ Test case Design Techinques

Used to prepare data for testing.
1) Reduce the data
2) More coverage

Types:
1.Equivalence Class Partitioning (ECP) 
2.Boundary Value analysis (BVA) 
3.Decision Table based testing 
4.State Transition 
5.Error Guessing
---
Ad 1. Partition data into various classes and we can select data according to class than test. It reduce the numer of test-cases and saves time for test.

Value check
Classify/devide/partition the data in to multiple classes

Exp.
Box allow digits 1 - 500

Normal tesing 1, 2, 3.....500

Devide values into Eqivalence Classes  -100 to 0 > -50 (invalid), 1-100 > 30 (valid), 101-200 > 160 (valid), 201-300 > 250 (valid), 301-400 > 320 (valid), 401-500 > 450 (valid), 501-600 > 550 (invalid)

Test data using ECP: -50, 30, 160, 250, 320, 450, 550
---

Ad 2. BVA technique used to check Bounbaries of the input.

Boundary of the inputs.

Min, Min+1, min-1
Max, Max+1, Max-1

Exp. Box allow digits 18-35.

Min = 18 (pass) Max = 35 (pass)
Min-1=17 (fail) Max-1=34 (pass)
Min+1=19 (pass) Max+1=36 (fail)
---

Input domain testing
The value will be verified in the text box/input fileds.
We use ECP & BVA
---

Ad 3. Decision Table is also called as Couse-Effect Table.
This technique will be used if we have more conditions and corresponding actions.
In Decision table technique, we deal with combinations of inputs.
To identify the test cases with decision table, we consider conditions and actions.

If we have more numer of conditions/actions than we use this technique.

Exp.
Take an example of transferring money online to account which is already added and approved.

Conditions to transfer money:
-Account already approved
-OTP (one time password) matched
-Sufficient money in the account

Actions performed:
-Transfer money
-Show a message as insufficient amount
-Block the transaction incase of suspicion transaction

						TC1	TC2	TC3	TC4	TC5
Con1 Account already approved			T	T	T	T	F
Con2 OTP Mached					T	T	F	F	X
Con3 Sufficient money in account		T	F	T	F	X
Act1 Transfer money				EXC.
Act2 Show message "insufficeient amount"		EXC.
Act3 Block transaction - suspicios trans.			EXC.	EXC.	X
						poz TC	Neg TC	NTC	NTC	Inv. TC
----

Ad 4. In State Transition technique changes in input conditions change the state of the application.
This technique allows tester to test the behawior of an AUT.
Tester can perform this action by entering various input conditions in a sequence.
In State transition the testing team provides positive as well as negativ input test values for evaluating the system behawior.

Exp. Login page which locks the user name after tree wrong attempts of password.

State	Login			Correct Password	Incorrect password
S1 	First Attempt		S4			S2
S2	Second Attempt		S4			S3
S3	Third Attempt		S4			S5
S4	Home Page
S5	Display a message as "Account Locked, please consult Administrator"
------
Ad 5.
Error guessing is one of the techniques used to find bugs in a software application based on tester's prior expirience.
In Error guessing we don't follow any specific rules.
It depends on Tester Analitical skills and expirience.
Exp.:
Submitting a form without entering values.
Entering invalid values such as entering a;phabets in the numberic fileds.
-------------
SDLC - Software Development Life Cycle
Req. Analysis > Design > Coding > Testing > Deployment > Maintnance

STLC - Software testing live cycle
1) Requirement Analysis
2) Test Palnning
3) Test Desing
4) Test Execution
5) Defect/Bug Reporting & Tracking
6) Test Closure

STLC
SNO	Phase		Input		Activities		Responsibility	Out come
1	est planning	Project Plan	Identify the resorces	Test/Team Lead70%  Test
		What?	Functional req.	Team formation		Test Manager 30%    Plan
		How?			Test estimation				Document
		When?			Preparation of test plan
					Reviews on Test Plan
					Test Plan Sign-off

2	Test Designing	Project Plan	Prep. of T.Scenaros	Test/Team Lead 30%  Test
			Funct. Req.	Prep. of T.Cases	Test Engineers 70%  Case
			Test Plan	Review on T.Cases			    Doc.
			Design Docs	Traceability Matrix			
			Use Cases	Test Cases Sign-off			T.Matrix

3	The Execution	Funct. Req.	Executing test cases	Test/Team Lead 10%  Status
			Test Plan	Prep. of T.Raport/Log	Test Engin. 90%	    T.Repo.
			Test Cases	Identifying defects
			Build from Dev team

4	Defect Report 	Test Cases	Prep. Defect. Raport	Test/Team Lead 10%  Defect
	& Tracking	T. Raports/log	Raport defects to Dev	Test Engin. 90%	    Raport

5	Test Closure	T.Raports	Analys. T. Reports	T.Lead/Manager 70%  Test
	/Sign-off	Def. Raports	Analys. Bug Reports	T. Engineers   30%  Summary
					Evaluating Exit Criteria		    Raports
------------------

Test Plan
T. Plan is a document that describes the test scope, test strategy, objectives, schedule, deliverables and resources required to perform testing for a software product.

Test Plan template contents:
Overview
Scope
-Inclusions
-Test Environments
-Exclusions
Test strategy
Defect raporting Procedure
Roles/Responsibilities
Test Schedule
Test Deliverables
Pricing
Entry and Exit Criteria
Suspension and Resumption Criteria
Tools
Risk and Mitigations
Approvals
-----------

Use case, Test Scenario & Test Case

Use Case:
Use case describes the requirements.
Use case contains THREE Items.
- Actor, which is the user, which can be a single person or group of people, interacting with a proces.
- Action, which is to reach the final outcome.
- Goal/Outcome, which is the succesful user outcome.

Test Scenario:
A possible area to be tested (What to test)

Test Case:
Step by step actions to be performer to validate functionality of AUT (How to test)
Test case contains test steps, expected result & actual result.
---

Use case vs Test case

Use Case - Describe functional requirement, prepared by Buisness Analyst (BA)
Test Case - Describes Test Steps/Procedure, prepared by Test Engineer.
--

Test Scenario vs Test Case
Test Scenario is "What to be tested" and Test Cases "How to be tested"

Exp.
Test Scenario: Checking the functionality of Login buton
- TC1: Click the buton without entering user name and password.
- TC2: Click the buton only entering User name.
- TC3: Click the buton while entering wrong user name and wrong password.
-----

Test Suite is group of test cases which belongs to came category.

---

A test case is a set of actions executeg to validate particular feature or fuctionality of your software application.

Test Case Contents:
Test Case ID
Test Case Title
Description
Pre-condition
Priority (P0, P1, P2, P3) - Order
Requirement ID
Steps/Actions
Expected Result
Actual Result
Test Data
------------

Requirement Tracebility Matrix (RTM)

RTM describes the mapping of Requirement's with Test Cases.
The main Purpose of RTM is to see that all test cases are covered so that no functionality should miss while doing Software testing.

RTM include:
Requirement ID
Req Description
Test case ID's
Status (can be)
------

Test Environment - is a platform specially build for test case execution on the software product.
It is created by integrating the required software and hardware along with proper network configuration.
Test environment simulates producttion/real time environment.
Another name of test environment is Test Bed.
------

Test execution

During this phase test team will carry out the testing based on the test plans and the test cases prepared.

Entry Criteria: Test cases, Test Data & Test Plan

Activities:
Test cases are executed based on the test planning.
Status of test cases are marked, like Passed, Failed, Blosked, Run, and others.
Documentation of test results and log defects for filed cases is done.
All the blocked and filed test cases are assigned bug ids.
Retesting once the defects are fixed.
Defects are tracked till closure.

Deliverables: Provides defect and test case execution report with completed results.
------

Guidelines for Test Execution:

The Build being deployed to the QA environment is the most importent part of the test execution cycle.
Test execution is done in Quality Assurence (QA) environment.
Test execution happens in multiple cycles.
Test execution phase consists Executing the test cases + test scripts ( if automation).
----

Defect/Bug

Any mismatched functionality found in a application is called as Defect/Bug/Issue.
During Test Execution Test engeneers are reporting mismatches as defects to developers through templates or using tools.

Defect Reporting Tools:

Clear Quest
DevTrack
Jira
Quality Center
Bug Jilla etc.
---------

Defect Report Contents:

Defect ID - Unique identification numer for the defect.
Defect Description - Detiled description of the defect including information about the module in which defect was found.
Version - Version of the application in which defect was found.
Steps - Detalied staps along with the screenshots with which the developer can reproduce the defects.
Data Raised - Date when the defect is raised.
Reference - where you Provide reference to the documents like requirements, design, archtecture or may be even screenshots of the error to help understand the defect.
Detected By - Name/ID of the tester who raised the defect.
Status - Status of the defect, more on this later.
Fixed by - Name/ID of the developer who fixed it.
Date Closed - Date when the defect is closed.
Severity - describes the impact of the defect ot the application.
Priority - is related to defect fixing urgency. Severity Priority could by High/Midium/Low based on the inpact urgensy at which the defect should be fixed respectively.
---

Defect Catigorization

Severity			Priority

Blocker				P0
Critical			P1
Major				P2
Minor				
----

Severity describes the seriusness of defect and how much inpact on Business workflow.

Defect severity can de categorized into four class:

Blocker - (Show stoper) This defect indicates nothing can proceed further.
Exp. Application crashed, Login Not worked.
Critical: The main/basic functionality is not working. Customer business workflow is broken. They cannot proceed further.
Exp1. Fund transter is not working in net banking.
Exp2. Ordering product in ecommerce application is not working.
Major - It couse some undesirable behawior, but the feature/application is still functional.
Exp1. After sending email there is no confirm message.
Exp2. After booking cab there is no confirmation.
Minor: It won't couse any major break-down of the system.
Exp. Look and feel issues, spellings, alignments.
------

Defect Priority
Describes the inportance of defect.
Defect Priority states the order in which a defect should ge fixed.

Defect priority can be categorized into three class:

P0(Height)- The defect must be resolved immidietly as it affect severity and cannot be used until it is fixed.
P1(Medium)- It can wait until a new versions/builds is created.
P2(Low)- Developer can be fix it in later releses.

Priority can be changed, but severity can not. Priority decided by tester, developer, biseness owner.
----

High/Low Severity/Priority

					Priority
			High					Low

	High	Login is taking to blank page.	About us link is going to blank page.
Severity
	Low	After user logged into		User opened contact page email id has
		application, he can see Home	spelling mistake.
		Page. But there is spelling
		mistake in Home Page.
-----------------

Defect Resolution
After reciving the defect report from the testig team, develoment team conduct a review meeting to fix defects. Then they send a resolution type to the testing team further communication.

Resolution Types:
Accept
Reject
Duplicate
Enhencment
Need more information
Not Reproducible
Fixed
As Designed
----------------

Bug Life Cycle

Tester finds a Bug > Status:New > Dev Project Manager Analyses The bug > Valid?
- No > Status: Rejected (Rejest resons: Enhancement, Need more info, Not reprod., As desig)
- Yes > Existing?
- Yes > Status:duplicate
- No > Delayed?
- Yes > Stetus: Deferred
- No > Status Assigned > Dev starts fixing bug > Status: Open > Dev fixed bug > status: fixed > Tester retest the defect >
- Pass > Status: Closed > Regression Testing > Fail > Reopening (Back to...status: open)
- Fail > Reopening (Back to...status: open)
---------

Test Cycle Closure

Activities
- Evaluate cycle completion criteria based on Time, Test coverage, Cost, Software, Critical Buisness Objectives, Quality
- Prepare test metrics based on the above parametrs.
- Document the learning out of the project.
- Prepare Test summary report
- Qualitive and quantitive reporting of quality of the work product to the customer.
- Test result analysis to find out the defect distribution by type and severity.

Deliverables
- Test Closure report
- Test Metrics
-------------

Test Metrics
1 No. of requirements
2 Avg. No. of test cases written per requirement
3 Total No. of test cases written for all requirement
4 Total No. of test cases executed
5 No. of test cases Passed
6 No. of test cases failed
7 No. of test cases blocked
8 No. of test cases unexecuted
9 Total No. of defects identified
10 Critical Defect count
11 Higher Defect count
12 Medium defects count
13 Low defects count
14 Customer Defects
15 No. of defects found in UAT
---

T. Metrics

Defect Density: No. of defects identified per requiremant/s
Defect Remuval Efficiency (DRE)
Defect Leakage
Defect Rejection Ratio
Defect Age
Customer satisfaction
---------

QA/Testing Activities

Understending the requirements and functional specifications of the application.
Identifying required Test Scenario's.
Designing Test Cases to validate application.
Setting up Test Equirement(Test Bed)
Execute Test Cases to valid application.
Log test results ( How many test cases pass/fail )
Defect reporting and tracking.
Retest fixed defects of previous build.
Perform various types of testing's in application.
Reports to Test Lead about the status of assigned tasks.
Participated in regular team meetings.
Creating automation scripts.
Provides recommendation on whether or not the application/system is ready for production.
-------

7 Principales of software Testing

1. Start software testing at early stages. Means from the begining when you get the requirements.
2. Test the software in order to find the defects.
3. Higly impossible to give the bug free software to the customer.
4. Should not do Exhausitive testing. Means we should not use same type of data for testing every time.
5. Testing is context base. Means decide what types of testing should be conducted based on type of application.
6. We should follow the concept of Pesticide Paradox. Means, if you are executing same cases for longer run, they wont be find any defects. We have to keep update test cases in every cycle/release in order to find more defects.
7. We should follow detected clustering. Means some of the modules contains most of the defects. By expirience, we can identify such risky modules. 80% of problems are found in 20% od the moduls.
-------

Module 2

Manual Testing Project
---
Project introduction
Understanding & Explore the Functionality (FRS)
Estimation
Test Plan
Wwiting Test scenarios
Writing Test Cases & Reviews (adding versions)
Environment Setup & Build deployment
Test Execution
Bug Reporting & Tracking
Sanity Testing, Re-testing & Regression Testing
Test sign off

Project introdution
---
Product
eCommerce product/Application

eCommerce
---
Login
Search for product/items
Add then to cart
Do payment
Prodyct will be deliverd
Returns the product
etc...

Frontend - public
Backend - Admins

URL: demo.opencart.com

Zapoznałem się z test planem.
Zapoznałem się z plikiem ze scenariuszami.


Relese Note - how to open (environment) and what is in build

1) MySQL Database
2)Apache server

XAMMP

https://localhost/phpmyadmin
http://localhost/opencart/upload/
http://localhost/opencart/upload/admin/
-----------------------------------------------

Module 3: Agile + Jira Tool

Agile model/metodology/proces
It is an itrativ and incremental approch. (repeating and adding)

Customer not need to wait for long time.
We develop, test and release peice if software to the customer with few numeber of features.
We can accept/accomodate requirement changes.


There will be good communication between Customer, Buisness Analyst, Developer & Testers.

Advantages:
---
Requirements changes are allowed in any stage of development (or) we can accommodate requirement changes in the middle of development.
Releases will be very fast (Weekly)
Customer no need to wait for long time.
Good communication between team.
It is very easy model to adopt.

Disadvantages:
Less Focus on design and documentation since we deliver software very fast.


Scrum
--------

Scrum is a freamwork through witch we build software product by following Agile Principales.
Scrum includes group of people called as Scrum team. Normally contains 5-9 members.

Scrum Team
--
Product Owner
Scrum Master
Dev Team
QA Team

Product Owner:
Define the features of the product
Prioritize features according to market value
Adjust features and priority every iteration, as needed
Accept or reject work results.

Scrum Master:
The main role is facilitating and driving the agile process.

Developers and QA:
Develop and test the software.


Scrum Terminology
---
User Story: A feature/module in a software.
- Epic: Collection of user stories.
- Product backlog: Contains list of user stories. Prepared by product owner. (document - list - exel)
- Sprint/Iteration: Period of time to complite the user stories, decidet by the product owner and team, usually 2-4 weeks of time.
- Sprint planning meeting: Meeting conducts with the team to define what can be delivered in the sprint and duration.
- Sprint backlog: List of committed stories by Dev/QA for specific sprint.
- Scrum meeting: Meeting conducted by Scrum Master everyday 15 mins. Called as scrum call/Standup meeting.
What did you do yesterday?
What will you do today?
Are there any impediments in your way?
- Sprint retrospective meeting after comletion of sprint. The entire team, including both the Scrum Master and the product owner should participate.

- Story point: Rought estimation of user stories, will be given by Dev & QA in the form of Fibonacci series.

0 1 1 2 3 5 8....
1 story point = 1 hour/ 1 day(6hours)
Login --> Dev - 5 QA - 3 = 8 hours/ 1 day

Burndown chart: Shows how much work remining in the sprint. Maintained by the scrum master daily.
-----------------------------------

Definition of ready & definitione of done

Definition of ready (DoR)
---
User Story is clear
User Story is testable
User story is feasible
User Story defined
User Story Acceptance Csiteria defined
User Story dependencies identified
User Story sized by Development Team
Scrum Team accepts User Experience artefacts
Performence criteria identified, where appropriete
Team has a good idea what it will mean to Demo the User Story

Definition of Done (DoD)
---
Code produced (all 'to do' items in code completed)
Code commented, checked in and run against current version in souce control
Peer reviewed (or produced with paif programming) and meeting development standards
Builds without errors
Unit test written and passing
Deployed to system test environment and passed system tests
Passed UAT (User Acceptance Testing) and signed off as meeting requirements
Any build / documented / communicated
Relevant documentation / diagrams produced and / or updated
Remaining hours for task set to zero and task closed

--------------
